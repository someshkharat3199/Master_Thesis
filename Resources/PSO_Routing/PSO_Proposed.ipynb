{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtIQ1Um3MgrX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import copy\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from scipy.stats import qmc\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "1MdL2DJyMsSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4adc53a-8126-41a8-ec2c-120b3092015d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(datetime.now().timestamp())"
      ],
      "metadata": {
        "id": "4BFp4nWtMtjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gdrive/MyDrive/Networks/test_cases.zip -d /"
      ],
      "metadata": {
        "id": "XNbRmj2xMu7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_nodes = [20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
        "test_case_num = 30\n",
        "\n",
        "#defining network parameters\n",
        "GATEWAY_ID = 0\n",
        "spread_factor = range(7,13)"
      ],
      "metadata": {
        "id": "nULulbECMw7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing particle population (priority and velocity vector of each particle) randomly\n",
        "number_particles = 30\n",
        "init_angle = (-180, 180)\n",
        "init_angle_velocity = (-5,5)\n",
        "init_priority = (-1000,1000)\n",
        "init_velocity = (-100,100)\n",
        "max_iterations = 500\n",
        "large_negative_value = -10**6\n",
        "large_positive_value = 10**6\n",
        "C = 0.5\n",
        "alpha = 1\n",
        "beta = 2\n",
        "c1 = 2.05\n",
        "c2 = 2.05\n",
        "phi = c1 + c2\n",
        "constriction_factor = 2/abs(2-phi-math.sqrt(phi**2-4*phi))"
      ],
      "metadata": {
        "id": "Hg9RLhTAMzHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Particle:\n",
        "  def __init__(self,angle, sf_priority, angle_velocity, sf_velocity ,fitness):\n",
        "    self.angle = angle\n",
        "    self.sf_priority = sf_priority\n",
        "    self.angle_velocity = angle_velocity\n",
        "    self.sf_velocity = sf_velocity\n",
        "    self.fitness = fitness\n",
        "\n",
        "  def __str__(self):\n",
        "    return f'''\n",
        "               angle          = {self.angle}\\n\n",
        "               sf_priority    = {self.sf_priority}\\n\n",
        "               angle_velocity = {self.angle_velocity}\\n\n",
        "               sf_velocity    = {self.sf_velocity}\\n\n",
        "               fitness        = {self.fitness}\\n\n",
        "            '''"
      ],
      "metadata": {
        "id": "LW8jMRAKM1eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dist(a,b):\n",
        "  x1,y1 = a\n",
        "  x2,y2 = b\n",
        "  return math.sqrt((x2-x1)**2+(y2-y1)**2)"
      ],
      "metadata": {
        "id": "gTmdfGacM4Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path, isGraph):\n",
        "  if isGraph:\n",
        "    G = nx.read_gml(path,destringizer=int)\n",
        "    return G\n",
        "  with open(path,'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "    file.close()\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "p0AWF1OMM642"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_path(G, edge_nodes, particle_position):\n",
        "  N_DEVICES = len(G.nodes)\n",
        "  ans = {}\n",
        "  for f in edge_nodes:\n",
        "    visited_nodes = [False for i in range(N_DEVICES)]\n",
        "    iteration = 0\n",
        "    terminal_node, sf = (f, max(particle_position.sf_priority[f], key = lambda x: particle_position.sf_priority[f][x]))\n",
        "    path = []\n",
        "    while terminal_node != GATEWAY_ID and iteration <= N_DEVICES:\n",
        "      visited_nodes[terminal_node] = True\n",
        "      iteration = iteration + 1\n",
        "      closest_node = -1\n",
        "      smallest_dist = large_positive_value\n",
        "      for adj_node_id in G.adj[terminal_node]:\n",
        "        if visited_nodes[adj_node_id]:\n",
        "          continue\n",
        "        \n",
        "        #get adjecent node coordinates\n",
        "        x0,y0 = G.nodes()[adj_node_id]['pos']\n",
        "\n",
        "        #get terminal node coordinates\n",
        "        Px,Py = G.nodes()[terminal_node]['pos']\n",
        "\n",
        "        #get terminal node angle (convert to radians)\n",
        "        theta = math.radians(particle_position.angle[terminal_node])\n",
        "\n",
        "        #calculate distance\n",
        "        distance = abs(math.cos(theta)*(Py-y0)-math.sin(theta)*(Px-x0))\n",
        "\n",
        "        #find the closest point\n",
        "        if distance < smallest_dist:\n",
        "          smallest_dist = distance\n",
        "          closest_node = adj_node_id\n",
        "        \n",
        "      if closest_node == -1:\n",
        "        break\n",
        "\n",
        "      next_node, next_sf = (closest_node, max(particle_position.sf_priority[closest_node], key = lambda x: particle_position.sf_priority[closest_node][x]))\n",
        "      \n",
        "      path.append((terminal_node, next_node, sf))\n",
        "      terminal_node = next_node\n",
        "      sf = next_sf\n",
        "    ans[f] = path\n",
        "  return ans\n",
        "      "
      ],
      "metadata": {
        "id": "dK_6hw7yM79t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fitness(particle_position, iteration, edge_nodes, G, network_data):\n",
        "  fitness_cost = 0\n",
        "  path = decode_path(G, edge_nodes, particle_position)\n",
        "  for f in edge_nodes:\n",
        "    #adding a penalty if invalid path is returned\n",
        "    if len(path[f]) == 0 or path[f][-1][1] != GATEWAY_ID:\n",
        "      fitness_cost = fitness_cost + large_positive_value\n",
        "      continue\n",
        "\n",
        "    #Calculate cost of path and adding link capacity penalty\n",
        "    total_delay = 0\n",
        "    for i,j,k in path[f]:\n",
        "      fitness_cost = fitness_cost + network_data['cst'][f,i,j,k]\n",
        "      if network_data['max_edge_data_rate'][k]-network_data['data_rate'][f,i,j,k] < 0:\n",
        "        fitness_cost = fitness_cost + network_data['cst'][f,i,j,k] + ((C*iteration)**alpha)*((network_data['max_edge_data_rate'][k]-network_data['data_rate'][f,i,j,k])**beta) \n",
        "      total_delay = total_delay + network_data['edge_delays'][f,i,j] + network_data['tx_time'][k] + network_data['eh_time'][i,k]      \n",
        "\n",
        "    #Adding delay constraint penalty\n",
        "    if network_data['max_flow_delays'][f]-total_delay < 0:\n",
        "      fitness_cost = fitness_cost + ((C*iteration)**alpha)*((network_data['max_flow_delays'][f]-total_delay)**beta)\n",
        "  \n",
        "  particle_position.fitness = fitness_cost"
      ],
      "metadata": {
        "id": "xlfT9De6NATc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_angle_values(G):\n",
        "  queue = [GATEWAY_ID]\n",
        "  N_DEIVCES = len(G.nodes)\n",
        "  visited = [False for node in range(N_DEVICES)]\n",
        "  visited[GATEWAY_ID] = True\n",
        "  angle_values = {}\n",
        "  angle_values[GATEWAY_ID] = 0\n",
        "  while len(queue) != 0:\n",
        "    for i in range(len(queue)):\n",
        "      front = queue.pop(0)\n",
        "      x1,y1 = G.nodes[front]['pos']\n",
        "      for adj_nodes in G.adj[front]:\n",
        "        if not visited[adj_nodes]:\n",
        "          visited[adj_nodes] = True\n",
        "          x2,y2 = G.nodes[adj_nodes]['pos']\n",
        "          if x1 == x2:\n",
        "            angle_values[adj_nodes] = 90.0\n",
        "          else:\n",
        "            angle_values[adj_nodes] = math.degrees(math.atan((y2-y1)/(x2-x1)))\n",
        "          queue.append(adj_nodes)\n",
        "  return angle_values"
      ],
      "metadata": {
        "id": "kZ9Ngj2aNDbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def heur_init(G):\n",
        "  N_DEVICES = len(G.nodes)\n",
        "  particle = []\n",
        "  angle_values = node_angle_values(G)\n",
        "  return [Particle(\n",
        "      {u: angle_values[u] + random.randint(-5,5) for u in range(N_DEVICES)},\n",
        "      {u: {k : random.randint(*init_priority) for k in spread_factor} for u in range(N_DEVICES)},\n",
        "      {u: 0 for u in range(N_DEVICES)},\n",
        "      {u: {k : 0 for k in spread_factor} for u in range(N_DEVICES)},\n",
        "      0\n",
        "  ) for p in range(math.ceil(number_particles*0.1))]"
      ],
      "metadata": {
        "id": "n0tF6SS3NFjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PSO_Algorithm(edge_nodes, G, network_data):\n",
        "  start = time.time()\n",
        "  end = time.time()\n",
        "  converge_iter = 0\n",
        "  N_DEVICES = len(G.nodes)\n",
        "\n",
        "  #particle initialization (Halton)\n",
        "  sampler = qmc.Halton(d = N_DEVICES, scramble=False)\n",
        "  sampler1 = qmc.Halton(d = 6, scramble=False)\n",
        "  angles = sampler.integers(l_bounds=-180, u_bounds=180, n = math.floor(number_particles*0.9), endpoint = True).tolist()\n",
        "  priorities = sampler1.integers(l_bounds = -1000, u_bounds = 1000, n = math.floor(number_particles*0.9)*N_DEVICES, endpoint = True).tolist()\n",
        "\n",
        "  particle = [Particle(\n",
        "      angles[p],\n",
        "      [{spread_factor[k]:priorities[p*N_DEVICES+u][k] for k in range(6)} for u in range(N_DEVICES)],\n",
        "      {u : 0 for u in range(N_DEVICES)},\n",
        "      {u : {k : 0 for k in spread_factor} for u in range(N_DEVICES)},\n",
        "      0\n",
        "  ) for p in range(math.floor(number_particles*0.9))]\n",
        "\n",
        "  #Particle initialization (heuristic initialization)\n",
        "  particle = particle + heur_init(G)\n",
        "\n",
        "  pBest = copy.deepcopy(particle)   #initially each particle is best\n",
        "  gBest = copy.deepcopy(particle[0]) #initially assume first particle as global best \n",
        "\n",
        "  calculate_fitness(gBest, 0, edge_nodes, G, network_data)\n",
        "\n",
        "  #calculate fitness of each particle and its neighbour\n",
        "  for p in range(number_particles):\n",
        "    calculate_fitness(particle[p],0,edge_nodes, G, network_data)\n",
        "    calculate_fitness(pBest[p], 0, edge_nodes, G, network_data)\n",
        "\n",
        "  #PSO Algorithm\n",
        "  for iteration in range(1,max_iterations+1):\n",
        "\n",
        "    #update pBest for each particle\n",
        "    for p in range(number_particles):\n",
        "      if particle[p].fitness < pBest[p].fitness:\n",
        "        pBest[p] = copy.deepcopy(particle[p])\n",
        "\n",
        "    #finding the global best\n",
        "    for p in range(number_particles):\n",
        "      if pBest[p].fitness < gBest.fitness:\n",
        "        gBest = copy.deepcopy(pBest[p])\n",
        "        converge_iter = iteration\n",
        "        end = time.time()\n",
        "\n",
        "    #calculate velocity\n",
        "    for p in range(number_particles):\n",
        "      for u in range(N_DEVICES):\n",
        "        r1, r2 = np.random.uniform(low = 0,high = 1,size = 2).tolist()\n",
        "        particle[p].angle_velocity[u] = round(constriction_factor*(particle[p].angle_velocity[u] + c1*r1*(pBest[p].angle[u]-particle[p].angle[u]) + c2*r2*(gBest.angle[u]-particle[p].angle[u])))\n",
        "        for k in spread_factor:\n",
        "          r1, r2 = np.random.uniform(low = 0,high = 1,size = 2).tolist()\n",
        "          particle[p].sf_velocity[u][k] = round(constriction_factor*(particle[p].sf_velocity[u][k] + c1*r1*(pBest[p].sf_priority[u][k]-particle[p].sf_priority[u][k]) + c2*r2*(gBest.sf_priority[u][k]-particle[p].sf_priority[u][k])))\n",
        "\n",
        "    #Update Positions\n",
        "    for p in range(number_particles):\n",
        "      for u in range(N_DEVICES):\n",
        "        particle[p].angle[u] = particle[p].angle[u] + particle[p].angle_velocity[u]\n",
        "        for k in spread_factor:\n",
        "          particle[p].sf_priority[u][k] = particle[p].sf_priority[u][k] + particle[p].sf_velocity[u][k]\n",
        "\n",
        "    #Calculate fitness of all particles\n",
        "    for p in range(number_particles):\n",
        "      calculate_fitness(particle[p],iteration,edge_nodes, G, network_data)\n",
        "\n",
        "  #decode solution\n",
        "  ans = decode_path(G, edge_nodes, gBest)\n",
        "\n",
        "  return [gBest.fitness, ans, converge_iter, (end-start)]"
      ],
      "metadata": {
        "id": "loYETjNgNIE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expected_ans = read_data(\"/content/gdrive/MyDrive/Networks/expected_ans.pkl\", False)"
      ],
      "metadata": {
        "id": "37I6_oAONTtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "route_optimality_ratio = {}\n",
        "near_route_optimality_ratio = {}\n",
        "route_failure_ratio = {}\n",
        "near_route_failure_ratio = {}\n",
        "results = {}\n",
        "percent_gap = {}\n",
        "iterations_to_converge = {}\n",
        "time_to_converge = {}\n",
        "invalid_tests = {}\n",
        "for N_DEVICES in num_of_nodes:\n",
        "  correct_solution = 0\n",
        "  nearly_correct = 0\n",
        "  total_tests = 0\n",
        "  invalid_graph = 0\n",
        "  results[N_DEVICES] = []\n",
        "  percent_gap[N_DEVICES] = []\n",
        "  iterations_to_converge[N_DEVICES] = []\n",
        "  time_to_converge[N_DEVICES] = []\n",
        "  for t in range(1,test_case_num+1):\n",
        "\n",
        "    #reading data\n",
        "    network_data = {}\n",
        "    G = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/graph.gml\",True)\n",
        "    network_data['tx_time'] = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/tx_time.txt\", False)\n",
        "    network_data['max_edge_data_rate'] = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/max_edge_data_rate.txt\",False)\n",
        "    network_data['cst'] = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/cst.txt\",False)\n",
        "    network_data['max_flow_delays'] = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/max_flow_delays.txt\",False)\n",
        "    network_data['edge_delays'] = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/edge_delays.txt\",False)\n",
        "    network_data['max_edge_delays'] = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/max_edge_delays.txt\",False)\n",
        "    network_data['data_rate'] = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/data_rate.txt\",False)\n",
        "    network_data['eh_time'] = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/eh_time.txt\",False)\n",
        "    network_data['init_res_energy'] = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/init_res_energy.txt\",False)\n",
        "    edge_nodes = read_data(f\"test_cases/num_nodes_{N_DEVICES}/test_{t}/edge_nodes.txt\",False)\n",
        "\n",
        "\n",
        "    #Get 2D coordinates of all the devices\n",
        "    coordinates = {i:G.nodes()[i]['pos'] for i in G.nodes}\n",
        "\n",
        "    #creating a list of bidirectional edges\n",
        "    edge_list = []\n",
        "    for u,v in G.edges:\n",
        "      edge_list.append((u,v))\n",
        "      edge_list.append((v,u))\n",
        "    if expected_ans[N_DEVICES][t] == -1:\n",
        "      invalid_graph = invalid_graph + 1\n",
        "      continue\n",
        "    optimal_cost, optimal_path, converge_iter, converge_time = PSO_Algorithm(edge_nodes, G, network_data)\n",
        "    optimal_cost = round(optimal_cost, 2)\n",
        "    print(f\"num_nodes = {N_DEVICES}, test_case = {t}, actual_cost = {optimal_cost}, optimal_cost = {expected_ans[N_DEVICES][t]}\")\n",
        "\n",
        "    time_to_converge[N_DEVICES].append(converge_time)\n",
        "    iterations_to_converge[N_DEVICES].append(converge_iter)\n",
        "    results[N_DEVICES].append([optimal_cost,expected_ans[N_DEVICES][t]])\n",
        "    percent_gap[N_DEVICES].append(((optimal_cost-expected_ans[N_DEVICES][t])/expected_ans[N_DEVICES][t])*100)\n",
        "    total_tests = total_tests + 1\n",
        "    if ((optimal_cost-expected_ans[N_DEVICES][t])/expected_ans[N_DEVICES][t])*100 <= 4:\n",
        "      nearly_correct = nearly_correct + 1\n",
        "    if optimal_cost == expected_ans[N_DEVICES][t]:\n",
        "      correct_solution = correct_solution + 1\n",
        "  \n",
        "  #calculating route optimality ratio\n",
        "  if total_tests == 0:\n",
        "    continue\n",
        "  invalid_tests[N_DEVICES] = invalid_graph\n",
        "  near_route_optimality_ratio[N_DEVICES] = nearly_correct/total_tests\n",
        "  near_route_failure_ratio[N_DEVICES] = (total_tests-nearly_correct)/total_tests\n",
        "  route_optimality_ratio[N_DEVICES] = correct_solution/total_tests\n",
        "  route_failure_ratio[N_DEVICES] = (total_tests-correct_solution)/total_tests"
      ],
      "metadata": {
        "id": "SNUKxuBFNVOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving results\n",
        "def save_file(file_name, results):\n",
        "  with open(\"/content/gdrive/MyDrive/Networks/\" + file_name, 'wb') as file:\n",
        "    pickle.dump(results, file)\n",
        "    file.close"
      ],
      "metadata": {
        "id": "NR75NGvqNNmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_file(\"PSO_ad2_route_optimality_ratio.pkl\", route_optimality_ratio)\n",
        "save_file(\"PSO_ad2_route_failure_ratio.pkl\", route_failure_ratio)\n",
        "save_file(\"PSO_ad2_results.pkl\", results)\n",
        "save_file(\"PSO_ad2_percent_gap.pkl\", percent_gap)\n",
        "save_file(\"PSO_ad2_iterations_to_converge.pkl\", iterations_to_converge)\n",
        "save_file(\"PSO_ad2_time_to_converge.pkl\", time_to_converge)"
      ],
      "metadata": {
        "id": "aQgggKfQaHKp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}